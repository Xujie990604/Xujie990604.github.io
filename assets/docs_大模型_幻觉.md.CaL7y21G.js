import{_ as t,c as o,o as n,ag as l}from"./chunks/framework.BDwTZuFy.js";const d=JSON.parse('{"title":"幻觉","description":"","frontmatter":{},"headers":[],"relativePath":"docs/大模型/幻觉.md","filePath":"docs/大模型/幻觉.md","lastUpdated":1757747049000}'),a={name:"docs/大模型/幻觉.md"};function i(s,r,e,g,h,p){return n(),o("div",null,r[0]||(r[0]=[l('<h1 id="幻觉" tabindex="-1">幻觉 <a class="header-anchor" href="#幻觉" aria-label="Permalink to &quot;幻觉&quot;">​</a></h1><h2 id="什么是大模型的-幻觉" tabindex="-1">什么是大模型的“幻觉”？ <a class="header-anchor" href="#什么是大模型的-幻觉" aria-label="Permalink to &quot;什么是大模型的“幻觉”？&quot;">​</a></h2><p>首先，要理解“幻觉”这个词是一个比喻。模型并没有像人类一样的主观意识或感知。<strong>“幻觉”指的是大模型在推理过程中，生成了看似合理、语法通顺，但实际上是错误的、捏造的、与现实世界事实不符，或者与其内部知识库不一致的信息。</strong></p><p>可以把大模型想象成一个知识渊博但有时会<strong>过于自信的实习生</strong>。他读过无数的书（训练数据），能流畅地遣词造句，但当他记不清或不知道某个细节时，他不会说“我不知道”，而是会根据他读过的所有内容的“文风”和“模式”，<strong>即兴编造一个最“像”正确答案的答案</strong>。</p><hr><h2 id="一、幻觉是如何产生的-根源剖析" tabindex="-1">一、幻觉是如何产生的？（根源剖析） <a class="header-anchor" href="#一、幻觉是如何产生的-根源剖析" aria-label="Permalink to &quot;一、幻觉是如何产生的？（根源剖析）&quot;">​</a></h2><p>幻觉的产生不是一个单一原因，而是多个因素共同作用的结果。其核心在于<strong>大模型的工作原理：它是一个“概率预测机器”，而不是一个“事实数据库”</strong>。它的根本任务是根据上文预测下一个最有可能出现的词。</p><h3 id="_1-训练数据层面的问题" tabindex="-1">1. 训练数据层面的问题 <a class="header-anchor" href="#_1-训练数据层面的问题" aria-label="Permalink to &quot;1. 训练数据层面的问题&quot;">​</a></h3><p>这是幻觉最根本的来源。</p><ul><li><strong>事实错误与噪声：</strong> 互联网充满了错误信息、过时的观点、小说情节和偏见。模型在训练时会把这些“脏数据”一并学习进去。例如，如果网上有大量科幻小说描写“火星上的古代城市”，模型可能会在回答关于火星的问题时，错误地引用这些信息。</li><li><strong>知识的局限性（Knowledge Cutoff）：</strong> 模型的知识是静态的，被“冻结”在训练数据截止的那个时间点。如果你问它关于最近发生的新闻，它无法从外部获取实时信息，只能基于旧有知识进行推测，这种推测极易出错。</li><li><strong>数据不平衡与偏见：</strong> 训练数据中反复出现的模式会被模型认为是“强关联”。例如，如果数据中大部分提到“护士”时都使用女性代词，模型就会在生成相关内容时强化这种性别偏见，即使这不符合事实。</li></ul><h3 id="_2-模型架构与目标函数的问题" tabindex="-1">2. 模型架构与目标函数的问题 <a class="header-anchor" href="#_2-模型架构与目标函数的问题" aria-label="Permalink to &quot;2. 模型架构与目标函数的问题&quot;">​</a></h3><p>模型的“天性”决定了它容易产生幻觉。</p><ul><li><strong>目标是“连贯”而非“真实”：</strong> 模型的优化目标（损失函数）是让生成文本的概率分布尽可能接近训练数据。简单来说，它被训练得更擅长<strong>生成流畅、语法正确、上下文连贯的句子</strong>，而不是确保句子的<strong>事实准确性</strong>。一个“听起来很对”的错误答案，在模型看来可能比一个“听起来有点绕”的正确答案得分更高。</li><li><strong>模式的过度泛化（Over-generalization）：</strong> 模型学习的是数据中的统计规律和模式。当它遇到一个模糊或未见过的问题时，它会套用一个它认为最相似的已知模式来“填空”，这个过程就容易导致错误信息的捏造。比如，它知道“XX 公司发布了 YY 产品”是一个常见模式，如果你问一个不存在的“A 公司发布 B 产品”，它很可能会编造一个发布会细节。</li><li><strong>编码-解码过程中的信息丢失：</strong> 在模型内部，语言被转换成高维度的数学向量。在这个抽象的表示空间中，细微但关键的事实差异可能会被模糊或丢失，导致解码生成文本时出现偏差。</li></ul><h3 id="_3-推理-解码-过程中的问题" tabindex="-1">3. 推理（解码）过程中的问题 <a class="header-anchor" href="#_3-推理-解码-过程中的问题" aria-label="Permalink to &quot;3. 推理（解码）过程中的问题&quot;">​</a></h3><p>这是在用户提问后，模型生成答案的环节。</p><ul><li><strong>解码策略的影响：</strong> 模型生成答案时，并非总是选择概率最高的那个词，否则回答会非常呆板。它会采用一些随机性策略（如 Temperature Scaling, Top-K/Top-p Sampling）来增加多样性和“创造性”。<strong>较高的“温度”（Temperature）会鼓励模型探索低概率的词，这虽然能产生更有趣的回答，但也极大地增加了产生幻觉的风险。</strong></li><li><strong>上下文理解不充分：</strong> 如果用户的提问含糊不清、有歧义或者包含错误的前提，模型可能会被误导。例如，你问：“请介绍一下牛顿在 1990 年发表的关于人工智能的论文”，模型可能会基于“牛顿”、“论文”等关键词，编造一篇不存在的论文，而不是指出牛顿早已逝世这个前提错误。</li></ul><hr><h2 id="二、如何避免或减轻幻觉" tabindex="-1">二、如何避免或减轻幻觉？ <a class="header-anchor" href="#二、如何避免或减轻幻觉" aria-label="Permalink to &quot;二、如何避免或减轻幻觉？&quot;">​</a></h2><p>这是一个系统性工程，需要从开发者和用户两端共同努力。</p><h3 id="a-开发者-研究者层面的策略" tabindex="-1">(A) 开发者/研究者层面的策略 <a class="header-anchor" href="#a-开发者-研究者层面的策略" aria-label="Permalink to &quot;(A) 开发者/研究者层面的策略&quot;">​</a></h3><ol><li><p><strong>提升数据质量：</strong></p><ul><li><strong>数据清洗与筛选：</strong> 在训练前，对数据进行更严格的过滤，剔除事实错误、仇恨言论和偏见内容。</li><li><strong>使用高质量数据集：</strong> 增加经过事实核查的、来源可靠的数据（如百科、科学文献、新闻档案）在训练集中的权重。</li></ul></li><li><p><strong>改进模型和训练方法：</strong></p><ul><li><strong>引用与溯源：</strong> 训练模型在回答时，明确指出其信息来源，并提供原始文本的链接。这让用户可以自行核实。</li><li><strong>知识增强（Knowledge Grounding）：</strong> 不让模型仅凭“记忆”回答，而是将其与外部知识库（如谷歌搜索、企业内部数据库）连接起来。这就是目前非常流行的 <strong>RAG (Retrieval-Augmented Generation，检索增强生成)</strong> 技术。 <ul><li><strong>RAG 工作流程：</strong> 用户提问 → 系统首先在外部知识库中检索相关信息 → 将检索到的信息作为上下文，连同原始问题一起喂给大模型 → 模型基于给定的可靠信息来生成答案。这极大地降低了模型“自由发挥”的空间。</li></ul></li><li><strong>模型对齐（Alignment）：</strong> 通过 <strong>RLHF (Reinforcement Learning from Human Feedback，基于人类反馈的强化学习)</strong> 等技术，让模型学习人类的偏好。在训练中，人类标注员会告诉模型哪些答案是好的（真实的、有用的、无害的），哪些是坏的（错误的、有幻觉的）。这相当于给模型上了一堂“价值观”课，教它更倾向于诚实。</li><li><strong>不确定性估计：</strong> 训练模型评估自己答案的置信度。当它对某个答案不确定时，它应该学会说“我不知道”或“根据现有资料，我无法确定”，而不是强行编造。</li></ul></li></ol><h3 id="b-用户层面的策略-非常重要" tabindex="-1">(B) 用户层面的策略（非常重要！） <a class="header-anchor" href="#b-用户层面的策略-非常重要" aria-label="Permalink to &quot;(B) 用户层面的策略（非常重要！）&quot;">​</a></h3><p>作为用户，我们不能盲目信任模型的输出。以下是你可以采用的最佳实践：</p><ol><li><p><strong>优化你的提问（Prompt Engineering）：</strong></p><ul><li><strong>提供充足的上下文：</strong> 你的问题越具体、背景信息越丰富，模型就越不容易跑偏。</li><li><strong>要求引用来源：</strong> 在提问时明确要求模型提供信息来源或出处。例如：“请解释一下什么是黑洞，并提供相关科学文献的参考。”</li><li><strong>使用角色扮演：</strong> 让模型扮演一个专家角色，并强调准确性。例如：“你现在是一位严谨的金融分析师，请基于 2023 年的数据分析...”。</li><li><strong>限制答案的范围：</strong> 不要问开放性过强的问题，可以要求它基于你提供的文本或数据进行回答。</li></ul></li><li><p><strong>批判性地审视答案（Critical Thinking）：</strong></p><ul><li><strong>交叉验证（Fact-Checking）：</strong> <strong>这是最重要的一点。</strong> 对于任何关键信息、数据、日期或事实，一定要通过可靠的第二来源（如权威网站、搜索引擎、书籍）进行核查。<strong>永远不要将大模型的回答直接当作最终事实。</strong></li><li><strong>注意“过于流畅”的陷阱：</strong> 幻觉往往隐藏在那些读起来非常通顺、自信满满的段落中。要对那些听起来“好到不真实”的细节保持警惕。</li></ul></li><li><p><strong>分解复杂问题：</strong></p><ul><li>将一个复杂的大问题分解成一系列简单、具体的小问题。这样不仅更容易验证每一步的答案，也能减少模型在长篇大论中产生幻觉的几率。</li></ul></li><li><p><strong>利用带有“接地”功能的工具：</strong></p><ul><li>优先选择那些已经集成了搜索引擎功能（即内置了 RAG）的 AI 工具，例如 Microsoft Copilot (Bing Chat)、Google Gemini (with Search extensions)、Perplexity AI 等。这些工具在回答时会主动检索网络信息，其答案的可靠性通常更高。</li></ul></li></ol><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>大模型的“幻觉”是其基于概率的生成式本质所固有的特性，目前还无法被彻底根除。它不是一个简单的“bug”，而是一个深刻的技术挑战。</p><p><strong>作为开发者</strong>，需要通过更高质量的数据、更先进的算法（如 RAG）和更负责任的对齐技术来构建“护栏”。 <strong>作为用户</strong>，则需要摒弃“把它当作全知全能的上帝”的想法，始终保持<strong>批判性思维和验证习惯</strong>，将其视为一个强大但需要监督的“助理”。</p>',27)]))}const c=t(a,[["render",i]]);export{d as __pageData,c as default};
